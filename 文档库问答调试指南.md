# 文档库问答调试指南

## 问题：AI 总是回复"抱歉"

如果文档库问答功能总是返回"抱歉，我无法生成回答"，请按以下步骤排查：

## 排查步骤

### 1. 检查控制台日志

打开开发者工具（F12），查看 Console 标签页的日志输出。

**正常的日志流程**：
```
RAG 问答开始: { question: "...", documentIds: [...], topK: 5 }
1. 向量化问题...
问题向量维度: 384
2. 检索相关文档块...
检索到文档块数量: 5
3. 构建上下文...
上下文长度: 2500 字符
4. 调用 AI 模型...
AI 模型响应: { model: "deepseek-r1:7b", choices: 1, hasContent: true }
AI 响应长度: 150 字符
```

### 2. 检查本地模型配置

#### 步骤 1：确认 Ollama 已安装

```bash
ollama --version
```

应该看到版本号，如：`ollama version is 0.1.x`

如果未安装，访问 https://ollama.com 下载安装。

#### 步骤 2：确认 Ollama 服务已启动

```bash
# 启动服务
ollama serve
```

或在应用设置中点击"启动 Ollama 服务"。

#### 步骤 3：确认模型已下载

```bash
# 查看已下载的模型
ollama list
```

应该看到类似：
```
NAME                    ID              SIZE    MODIFIED
deepseek-r1:7b         abc123def456    4.7 GB  2 days ago
```

如果没有，下载模型：
```bash
ollama pull deepseek-r1:7b
```

#### 步骤 4：测试模型

```bash
ollama run deepseek-r1:7b "你好"
```

应该得到正常回复。

### 3. 检查应用配置

打开设置 → 本地模型：

1. **使用本地模型**：✅ 已启用
2. **Ollama 地址**：`http://localhost:11434`
3. **模型名称**：`deepseek-r1:7b`（或你下载的模型）
4. **测试连接**：点击后应显示"连接成功"

### 4. 常见错误及解决方案

#### 错误 1：无法连接到 Ollama 服务

**日志**：
```
Error: ECONNREFUSED
❌ 无法连接到 Ollama 服务
```

**解决**：
1. 确认 Ollama 服务已启动：`ollama serve`
2. 检查端口是否被占用：`netstat -ano | findstr :11434`
3. 尝试重启 Ollama 服务

#### 错误 2：模型未找到

**日志**：
```
Error: model not found
❌ 本地模型请求失败: model not found
```

**解决**：
1. 检查模型名称是否正确（区分大小写）
2. 下载模型：`ollama pull deepseek-r1:7b`
3. 在设置中更新模型名称

#### 错误 3：AI 返回空内容

**日志**：
```
AI 模型响应: { model: "deepseek-r1:7b", choices: 1, hasContent: false }
❌ AI 模型返回了空内容
```

**可能原因**：
- 上下文太长，超过模型限制
- 模型加载失败
- Ollama 服务异常

**解决**：
1. 重启 Ollama 服务
2. 尝试更小的文档或更少的检索结果
3. 检查 Ollama 日志：查看 `用户数据目录/ollama.log`

#### 错误 4：未检索到文档块

**日志**：
```
检索到文档块数量: 0
抱歉，我在文档中没有找到相关信息
```

**可能原因**：
- 文档未正确索引
- 问题与文档内容不相关
- 相似度阈值过高

**解决**：
1. 检查文档是否上传成功（状态为"就绪"）
2. 尝试更具体或更宽泛的问题
3. 重新上传文档

### 5. 手动测试 API

使用 curl 或 Postman 测试 Ollama API：

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "deepseek-r1:7b",
  "prompt": "你好",
  "stream": false
}'
```

应该得到 JSON 响应，包含 `response` 字段。

### 6. 查看详细错误信息

在控制台中查找以下关键信息：

```javascript
// AI Service 配置
AI Service: 使用本地模型 deepseek-r1:7b @ http://localhost:11434

// 错误详情
Error details: {
  message: "...",
  code: "...",
  status: ...,
  localMode: true,
  model: "deepseek-r1:7b"
}
```

## 完整的测试流程

### 1. 测试 Ollama 服务

```bash
# 1. 检查安装
ollama --version

# 2. 启动服务
ollama serve

# 3. 在另一个终端测试
ollama run deepseek-r1:7b "测试"
```

### 2. 测试应用配置

1. 打开应用设置
2. 启用"使用本地模型"
3. 点击"测试连接"
4. 应该看到"连接成功"

### 3. 测试文档上传

1. 打开文档库
2. 上传一个 PDF 文件
3. 等待处理完成（状态变为"就绪"）
4. 检查控制台是否有错误

### 4. 测试问答

1. 选中一个文档
2. 输入简单问题："这个文档讲了什么？"
3. 查看控制台日志
4. 检查回答是否正常

## 性能优化建议

如果问答速度很慢：

1. **使用更小的模型**
   ```bash
   ollama pull mistral:7b  # 更快但质量稍低
   ```

2. **减少检索数量**
   - 修改 `ragService.ts` 中的 `topK` 参数
   - 从 5 改为 3

3. **使用 GPU 加速**
   - 确保有 NVIDIA 显卡
   - 安装 CUDA
   - Ollama 会自动使用 GPU

4. **增加系统内存**
   - 推荐至少 16GB RAM
   - 关闭其他占用内存的应用

## 获取帮助

如果以上步骤都无法解决问题：

1. **收集信息**：
   - 控制台完整日志
   - Ollama 版本
   - 模型名称和大小
   - 系统配置（OS、RAM、GPU）

2. **检查日志文件**：
   - 应用日志：开发者工具 Console
   - Ollama 日志：`用户数据目录/ollama.log`

3. **提交 Issue**：
   - 包含上述信息
   - 描述复现步骤
   - 附上错误截图

## 常见问题 FAQ

### Q: 为什么必须使用本地模型？

A: 文档库功能设计为完全本地化，保护隐私。所有文档内容和问答都在本地处理，不上传到云端。

### Q: 可以使用云端 API 吗？

A: 目前文档库仅支持本地模型。如果需要使用云端 API，可以在主聊天界面使用文档模式。

### Q: 推荐使用哪个模型？

A: 推荐 `deepseek-r1:7b`，平衡了速度和质量。如果硬件性能较弱，可以使用 `mistral:7b`。

### Q: 模型下载很慢怎么办？

A: Ollama 会从官方源下载，可能较慢。可以：
1. 使用代理
2. 等待下载完成（只需一次）
3. 从其他渠道获取模型文件

### Q: 可以使用其他嵌入模型吗？

A: 目前使用 `Xenova/all-MiniLM-L6-v2`。如需更换，需要修改代码并重新索引所有文档。

## 技术细节

### 工作流程

```
用户提问
  ↓
向量化问题 (embeddingService)
  ↓
检索相关文档块 (vectorStoreService)
  ↓
构建上下文
  ↓
调用 AI 模型 (aiService → Ollama)
  ↓
返回答案
```

### 关键参数

```typescript
// ragService.ts
topK: 5              // 检索前 5 个最相关的文档块
minSimilarity: 0.3   // 最小相似度阈值

// aiService.ts
temperature: 0.3     // 较低温度，更准确
max_tokens: 2000     // 最大回复长度
```

### 调试技巧

1. **启用详细日志**：
   - 已在代码中添加 `console.log`
   - 查看每个步骤的输出

2. **单独测试各组件**：
   - 测试嵌入模型：上传文档
   - 测试向量检索：查看检索结果数量
   - 测试 AI 模型：在主界面对话

3. **使用断点调试**：
   - 在 VS Code 中设置断点
   - 使用 F5 启动调试模式
   - 逐步执行代码

## 总结

大多数"抱歉"问题都是由于：
1. Ollama 服务未启动
2. 模型未下载
3. 配置不正确

按照本指南逐步排查，应该能解决问题。如果仍有问题，请查看控制台日志获取更多信息。

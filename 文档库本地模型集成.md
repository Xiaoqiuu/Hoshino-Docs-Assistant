# 文档库本地模型集成

## 更新日期
2024-11-25

## 概述

文档库的问答功能现已完全集成本地 Ollama 模型，实现完全离线的文档智能问答。

## 核心特性

### 1. 本地模型问答

**优势**：
- ✅ 完全离线运行，保护隐私
- ✅ 无需 API Key，零成本使用
- ✅ 响应速度快（取决于本地硬件）
- ✅ 数据不出本地，安全可靠

**工作流程**：
```
用户提问 → 向量检索 → 构建上下文 → 本地模型推理 → 返回答案
```

### 2. 智能上下文构建

**检索策略**：
- 使用嵌入模型将问题向量化
- 在文档向量库中检索最相关的 Top-K 片段
- 计算余弦相似度，过滤低相关内容
- 构建结构化上下文提供给模型

**上下文格式**：
```
[文档片段 1] (来自《文档名》第 X 页)
内容...

---

[文档片段 2] (来自《文档名》第 Y 页)
内容...
```

### 3. 专业的文档问答 Prompt

**System Prompt**：
```
你是 Hoshino，一个专业的文档助手。
请基于用户提供的文档内容回答问题，
并在回答中引用相关内容。
```

**User Prompt**：
```
请基于以下文档内容回答用户的问题。

文档内容：
[检索到的相关片段]

用户问题：[用户的问题]

请注意：
1. 只基于上述文档内容回答，不要编造信息
2. 如果文档中没有相关信息，请明确说明
3. 回答要准确、简洁、有条理
4. 可以引用文档片段来支持你的回答
```

## 技术实现

### 架构图

```
┌─────────────────────────────────────────────────────────────┐
│                      文档库界面                              │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐                  │
│  │ 文档1 ☑  │  │ 文档2 ☐  │  │ 文档3 ☐  │                  │
│  └──────────┘  └──────────┘  └──────────┘                  │
│                                                               │
│  [输入问题...                              ] [发送]          │
└─────────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────────┐
│                    RAG Service                               │
│  1. 向量化问题 (embeddingService)                           │
│  2. 检索相关片段 (vectorStoreService)                       │
│  3. 构建上下文                                               │
│  4. 调用 AI 模型 (aiService)                                │
└─────────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────────┐
│                    AI Service                                │
│  检查模式: localMode = true                                  │
│  ↓                                                            │
│  使用 Ollama API (http://localhost:11434/v1)                │
│  ↓                                                            │
│  调用本地模型 (如 deepseek-r1:7b)                           │
└─────────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────────┐
│                  Ollama 服务                                 │
│  本地运行的 LLM 推理引擎                                     │
│  支持多种开源模型                                            │
└─────────────────────────────────────────────────────────────┘
```

### 代码流程

#### 1. 用户提问

```typescript
// DocumentLibrary.tsx
const handleAsk = async () => {
  // 添加用户消息
  const userMessage: Message = {
    role: 'user',
    content: question,
  };
  setMessages(prev => [...prev, userMessage]);
  
  // 调用 RAG 问答
  const result = await window.electronAPI.ragAnswer(
    question, 
    Array.from(selectedDocs)
  );
  
  // 显示回答
  const assistantMessage: Message = {
    role: 'assistant',
    content: result.answer.answer,
    sources: result.answer.sources,
  };
  setMessages(prev => [...prev, assistantMessage]);
};
```

#### 2. RAG 处理

```typescript
// ragService.ts
async answer(question: string, documentIds?: string[]): Promise<RAGAnswer> {
  // 1. 向量化问题
  const questionEmbedding = await embeddingService.embed(question);
  
  // 2. 检索相关文档块
  const searchResults = vectorStoreService.search(
    questionEmbedding,
    documentIds,
    5,  // Top-K
    0.3 // 最小相似度
  );
  
  // 3. 构建上下文
  const context = searchResults
    .map((result, i) => {
      return `[文档片段 ${i + 1}] (来自《${result.chunk.documentName}》第 ${result.chunk.metadata.page} 页)\n${result.chunk.content}`;
    })
    .join('\n\n---\n\n');
  
  // 4. 调用 AI 模型
  const aiResponse = await aiService.chatWithContext(question, context);
  
  return {
    answer: aiResponse.response,
    sources: searchResults.map(r => ({
      documentId: r.chunk.documentId,
      documentName: r.chunk.documentName,
      page: r.chunk.metadata.page,
      content: r.chunk.content.slice(0, 200) + '...',
      similarity: r.similarity,
    })),
  };
}
```

#### 3. AI 服务调用

```typescript
// aiService.ts
async chatWithContext(message: string, context: string): Promise<AIResponse> {
  const modelToUse = this.localMode ? this.ollamaModel : this.model;
  
  const completion = await this.client!.chat.completions.create({
    model: modelToUse,
    messages: [
      {
        role: 'system',
        content: '你是 Hoshino，一个专业的文档助手。请基于用户提供的文档内容回答问题，并在回答中引用相关内容。',
      },
      {
        role: 'user',
        content: `文档内容：\n${context}\n\n问题：${message}\n\n请基于上述文档内容回答问题。`,
      },
    ],
    temperature: 0.3,  // 较低的温度，更准确
    max_tokens: 2000,
  });
  
  return {
    response: completion.choices[0]?.message?.content || '抱歉，我无法生成回答。',
    sources: [...],
  };
}
```

## 使用指南

### 前置条件

1. **安装 Ollama**
   ```bash
   # 访问 https://ollama.com 下载安装
   ```

2. **下载模型**
   ```bash
   # 推荐使用 deepseek-r1:7b（约 4.7GB）
   ollama pull deepseek-r1:7b
   
   # 或其他模型
   ollama pull qwen2.5:7b
   ollama pull llama3.1:8b
   ```

3. **启动 Ollama 服务**
   ```bash
   ollama serve
   ```
   
   或在应用设置中一键启动

4. **配置应用**
   - 打开设置 → 本地模型
   - 启用"使用本地模型"
   - 选择已下载的模型
   - 测试连接

### 使用步骤

1. **上传文档**
   - 点击"📄 上传文档"
   - 选择 PDF 文件
   - 等待处理完成（首次需下载嵌入模型）

2. **选择文档**
   - 点击文档卡片选中
   - 可以多选

3. **提问**
   - 在底部输入框输入问题
   - 点击"发送"或按 Enter

4. **查看回答**
   - 右下角弹出聊天窗口
   - 查看 AI 回答和来源引用
   - 可以继续提问

### 最佳实践

#### 1. 问题设计

**好的问题**：
- ✅ "文档中提到的主要观点是什么？"
- ✅ "如何配置 XXX 功能？"
- ✅ "第 5 页提到的数据是多少？"

**不好的问题**：
- ❌ "你觉得怎么样？"（太主观）
- ❌ "帮我写一篇文章"（超出文档范围）
- ❌ "这个"（缺乏上下文）

#### 2. 文档准备

**建议**：
- 文档内容清晰、结构化
- 避免扫描件（OCR 质量影响检索）
- 单个文档不超过 10MB
- 相关内容放在同一文档

#### 3. 模型选择

**推荐模型**：

| 模型 | 大小 | 速度 | 质量 | 适用场景 |
|------|------|------|------|----------|
| deepseek-r1:7b | 4.7GB | 快 | 高 | 通用文档问答 |
| qwen2.5:7b | 4.7GB | 快 | 高 | 中文文档 |
| llama3.1:8b | 4.7GB | 中 | 高 | 英文文档 |
| mistral:7b | 4.1GB | 快 | 中 | 轻量级场景 |

**硬件要求**：
- 最低：8GB RAM，集成显卡
- 推荐：16GB RAM，独立显卡
- 最佳：32GB RAM，NVIDIA GPU

## 性能优化

### 1. 检索优化

**参数调整**：
```typescript
// ragService.ts
const searchResults = vectorStoreService.search(
  questionEmbedding,
  documentIds,
  5,    // Top-K: 增加可获得更多上下文，但会变慢
  0.3   // 相似度阈值: 降低可获得更多结果，但可能不相关
);
```

**建议**：
- 一般问题：Top-K = 5, 阈值 = 0.3
- 精确查找：Top-K = 3, 阈值 = 0.5
- 广泛搜索：Top-K = 10, 阈值 = 0.2

### 2. 模型优化

**温度参数**：
```typescript
temperature: 0.3  // 文档问答建议使用较低温度
```

- 0.1-0.3：准确、保守（推荐用于文档问答）
- 0.5-0.7：平衡
- 0.8-1.0：创造性、发散

**Token 限制**：
```typescript
max_tokens: 2000  // 根据需要调整
```

### 3. 缓存策略

**向量缓存**：
- 文档向量持久化存储
- 避免重复计算

**模型缓存**：
- Ollama 自动缓存模型
- 首次加载较慢，后续快速

## 故障排除

### 问题 1：本地模型服务未连接

**症状**：
```
❌ 本地模型服务未连接
```

**解决方案**：
1. 检查 Ollama 是否安装
   ```bash
   ollama --version
   ```

2. 启动 Ollama 服务
   ```bash
   ollama serve
   ```

3. 在设置中测试连接

### 问题 2：模型未下载

**症状**：
```
❌ 本地模型请求失败: model not found
```

**解决方案**：
```bash
# 下载模型
ollama pull deepseek-r1:7b

# 验证
ollama list
```

### 问题 3：回答质量不佳

**可能原因**：
- 文档质量问题
- 检索参数不当
- 模型选择不合适

**解决方案**：
1. 检查文档内容是否清晰
2. 调整 Top-K 和相似度阈值
3. 尝试其他模型
4. 优化问题表述

### 问题 4：响应速度慢

**可能原因**：
- 硬件性能不足
- 模型太大
- 上下文太长

**解决方案**：
1. 使用更小的模型（如 mistral:7b）
2. 减少 Top-K 值
3. 升级硬件（增加 RAM 或使用 GPU）

## 隐私与安全

### 数据隐私

✅ **完全本地化**：
- 文档存储在本地
- 向量计算在本地
- 模型推理在本地
- 数据不上传云端

✅ **无需网络**：
- 模型下载后可离线使用
- 仅嵌入模型首次需要网络

### 数据安全

**存储位置**：
```
用户数据目录/
├── documents/          # 原始文档
├── rag-index.json     # 文档索引
└── data/
    ├── documents.db   # 文档元数据
    └── vectors.db     # 向量数据
```

**建议**：
- 定期备份数据目录
- 敏感文档使用加密存储
- 不要上传机密文档到云端

## 未来改进

### 短期（1-2周）
- [ ] 支持更多文档格式（Word、TXT、Markdown）
- [ ] 添加文档预处理选项
- [ ] 优化检索算法
- [ ] 添加问答历史记录

### 中期（1个月）
- [ ] 支持多轮对话上下文
- [ ] 添加文档摘要功能
- [ ] 支持文档对比分析
- [ ] 集成更多嵌入模型

### 长期（3个月）
- [ ] 支持图表和表格理解
- [ ] 添加知识图谱构建
- [ ] 支持多模态文档（图文混合）
- [ ] 实现增量学习

## 总结

文档库现已完全集成本地 Ollama 模型，实现了：

1. **完全离线**：无需 API Key，保护隐私
2. **智能检索**：基于向量相似度的精准检索
3. **专业问答**：针对文档优化的 Prompt 设计
4. **友好界面**：现代化的画廊视图和悬浮对话框

这使得 Hoshino 成为一个真正的本地化、隐私优先的文档智能助手。
